{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pprint import pprint\n",
    "import os\n",
    "from pprint import pprint\n",
    "from groq import Groq\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import dotenv\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "SESSION_TOKEN = os.getenv(\"SESSION_TOKEN\")\n",
    "REGION = os.getenv(\"REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIA4MTWHW4LILDFKOAZ\n",
      "UqgvNJ8KG8eu66n9LKyVoGXODNUUsmWNwVjdeFSG\n",
      "IQoJb3JpZ2luX2VjECwaCmFwLXNvdXRoLTEiRzBFAiEA9il2ei+2OutpUTLCgzeHgopw2Z0A8WTwZoVJvF/V09MCIApTFZtyRd5zbntJyQk6Uv1OFeTO2Uu3zB6Ltoud9q0lKqADCEUQABoMODUxNzI1MjM1OTkwIgxaYXPVcLbICIgQByAq/QLa4mYlPD91h/JxJNACXgAUaAIRo2IfVIQgfails1oH6CKrSnRNK8am2kgXgSiV2st3DLIMw87Aa2MMCtqyuFnKF3O5hFL0DMH0y8VXKAOKkhlzUatgJGTa+sOrHOyXW0HytYNvLa8PSmXp8NBl6CKJEr0Zm46rENP2GroZsi/F1aI1hP5kAew39c9oIniBYt07j/FmwrGbH8GcdI9oKEweAjH9iQ9KGYFGb1bX+trr7X3/hk1v5pqS9MOp/ZtKVQQLdxrPTh3RGQLRMxP5eO1wQmQPl09s8fl10bmbSpGI5yCubGTk5gV/YVyZoVFe9kCaJOefLtu7ee2KM59Pjjgu8FQczJvSlt8YwuLbAsnIulHYyiWYWfsaSTY199OAwvhBWIHg8aXJGNpyp6W7MmNTypJ9BaRzPlvxxgVk5mUbfJ4ione9+mKu1+wL0hr50MP4QK3g8kbL2c8WbCKDylwXlUfo48PUAs/Z6OAADUj/xpYfkFUOq3IbQ54d++Yw6b3BtgY6pgGRfpRFENhaqboSxaHNdMVCm9nzd5wxeDGXH8dor/OvMzAOkjnXAvenY3sdayIdkGFEbxYlQiTN3QtF13AbQ2F1sTDAi7hYNikfq8wxiE+ptxZqEgAI3ux7j/qYZrvxuz8P+rbvoH898QT9JBYoZPb75PeJ/+/MaNKpg5SWaJhWvGxxQ3OFO0nguDnwYAuMLMpy9+rTGKPYvOu5F17MlrEl7FNt0HTy\n"
     ]
    }
   ],
   "source": [
    "print(ACCESS_KEY)\n",
    "print(SECRET_KEY)\n",
    "print(SESSION_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transcribe_client = boto3.client(\n",
    "    'transcribe',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token = SESSION_TOKEN,\n",
    "    region_name=REGION\n",
    ")\n",
    "\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token = SESSION_TOKEN,\n",
    "    region_name=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcribed_files/better2.txt\n",
      "[]\n",
      "Folder output\\better2_0\n",
      "Video already exists in s3://interview-performance-analysis/videos/better2.mp4\n"
     ]
    }
   ],
   "source": [
    "# Local video details\n",
    "local_video_folder = 'input'\n",
    "video = 'better2.mp4'\n",
    "local_file_path = os.path.join(local_video_folder, video)\n",
    "\n",
    "# S3 details\n",
    "bucket_name = 'interview-performance-analysis'\n",
    "s3_video_folder = 'videos'\n",
    "s3_output_folder = 'transcribed_files'\n",
    "video_name = video.split('.')[0]\n",
    "s3_video_key = f'{s3_video_folder}/{video}'\n",
    "s3_output_file_key = f'{s3_output_folder}/{video_name}.txt'\n",
    "print(s3_output_file_key)\n",
    "\n",
    "# folder name folder creation if not already exists\n",
    "output_folder = \"output\"\n",
    "file_cnt = 0\n",
    "existing_folders = os.listdir(output_folder)\n",
    "filtered_folders = [folder for folder in existing_folders if folder.startswith(f\"{video_name}\")]\n",
    "sorted_folders = sorted(filtered_folders, reverse=True)\n",
    "\n",
    "print(sorted_folders)\n",
    "\n",
    "if len(sorted_folders) > 0:\n",
    "    cnt = (int)(sorted_folders[0].split('_')[-1])\n",
    "    file_cnt = cnt+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_folder = os.path.join(output_folder, f\"{video_name}_{file_cnt}\")\n",
    "\n",
    "os.makedirs(current_folder,exist_ok=True)\n",
    "print(f\"Folder {current_folder}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    s3.head_object(Bucket=bucket_name, Key=s3_video_key)\n",
    " \n",
    "    print(f'Video already exists in s3://{bucket_name}/{s3_video_key}')\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        # Upload video to S3 if it does not exist\n",
    "        try:\n",
    "            s3.upload_file(local_file_path, bucket_name, s3_video_key)\n",
    "            print(f'Successfully uploaded {local_file_path} to s3://{bucket_name}/{s3_video_key}')\n",
    "        except Exception as upload_error:\n",
    "            print(f'Error uploading file: {upload_error}')\n",
    "    elif error_code == '400':\n",
    "        print(f'Bad Request: Please check the bucket name, key, and permissions. Error: {e}')\n",
    "    else:\n",
    "        print(f'Error checking file existence: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcription(video_file_name):\n",
    "    media_file_uri = f's3://{bucket_name}/{s3_video_key}'\n",
    "    try:\n",
    "        transcription_job = transcribe_client.start_transcription_job(\n",
    "            TranscriptionJobName=f'interview-performance-analysis-{video_name}',\n",
    "            Media={'MediaFileUri': media_file_uri},\n",
    "            MediaFormat='mp4',\n",
    "            LanguageCode='en-US',\n",
    "            OutputBucketName=bucket_name,\n",
    "            OutputKey=s3_output_file_key,\n",
    "            Settings = {\n",
    "                'ShowSpeakerLabels': True,\n",
    "                'MaxSpeakerLabels': 2\n",
    "\n",
    "            }\n",
    "        )\n",
    "        print(\"Transcription job started successfully:\", transcription_job)\n",
    "    except Exception as e:\n",
    "        print(\"Error starting transcription job:\", e)\n",
    "    \n",
    "    while True:\n",
    "        job = transcribe_client.get_transcription_job(TranscriptionJobName=transcription_job['TranscriptionJob']['TranscriptionJobName'])\n",
    "        if job['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "            break\n",
    "\n",
    "    print(job['TranscriptionJob']['TranscriptionJobStatus'])\n",
    "    print(job['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_data(data):\n",
    "    sentence_wpm = {}\n",
    "    spk0_time = 0\n",
    "    spk1_time = 0\n",
    "    req_data = data[\"results\"][\"audio_segments\"]\n",
    "    conv_dict = {\n",
    "        'speaker': '',\n",
    "        'transcript':''\n",
    "    }\n",
    "    candidate_wpm = {\n",
    "        'wpm':0,\n",
    "        'transript':''\n",
    "    }\n",
    "    spk = 'spk_0'\n",
    "    conversation = {}\n",
    "    flag = False\n",
    "    text = ''\n",
    "    serial_number = 1\n",
    "    index = 0\n",
    "    for val in req_data:\n",
    "        \n",
    "        \n",
    "        \n",
    "        # pprint(val)\n",
    "        if val['speaker_label'] == \"spk_0\":\n",
    "            spk0_time += float(val['end_time']) - float(val['start_time'])\n",
    "        if val['speaker_label'] == \"spk_1\":\n",
    "            curr_time = (float(val['end_time']) - float(val['start_time']))/60\n",
    "            spk1_time += float(val['end_time']) - float(val['start_time'])\n",
    "            sentence_cnt = len(val['transcript'].split())\n",
    "            candidate_wpm = {\n",
    "                'wpm': sentence_cnt/curr_time ,\n",
    "                'transript':val['transcript']\n",
    "            }\n",
    "            sentence_wpm[index] = candidate_wpm\n",
    "            index+=1\n",
    "\n",
    "\n",
    "        if spk !=  val['speaker_label']:\n",
    "            conversation[serial_number] = {\n",
    "                \"speaker\": spk,\n",
    "                \"transcript\": text\n",
    "            }\n",
    "            # print(spk, text)\n",
    "            # print(\"check 1-->\",conversation[serial_number])\n",
    "            serial_number+=1\n",
    "            text= ''\n",
    "            spk = val['speaker_label']\n",
    "            flag = True\n",
    "            # print(\"after change-->\",spk)\n",
    "        if spk == val['speaker_label']:\n",
    "            # print(\" spk check-->\",spk)\n",
    "            text += val['transcript']\n",
    "            spk = val['speaker_label']\n",
    "            flag = False\n",
    "\n",
    "    \n",
    "    if flag == False:\n",
    "        conversation[serial_number] = {\n",
    "                \"speaker\": spk,\n",
    "                \"transcript\": text\n",
    "            }\n",
    "    # print(\"conversation ---->\",conversation)\n",
    "\n",
    "\n",
    "    conversation_file = f'{video_name}_conversation.txt'\n",
    "    output_file = os.path.join(current_folder,conversation_file)\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(json.dumps(conversation, indent=4))\n",
    "\n",
    "    return conversation, spk0_time, spk1_time,sentence_wpm \n",
    "                               \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output():\n",
    "    print(f\"Bucket: {bucket_name}, Key: {s3_output_file_key}\")\n",
    "    \n",
    "    # Fetch the file from S3\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=s3_output_file_key)\n",
    "    print(response)\n",
    "    \n",
    "    # Read and decode the file content\n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    data = json.loads(file_content)\n",
    "    \n",
    "    # Define output directory and video folder\n",
    "    raw_file = f'{video_name}_raw.txt'\n",
    "    output_file = os.path.join(current_folder,raw_file)\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(json.dumps(data, indent=4))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_data():\n",
    "\n",
    "    data = save_output()\n",
    "\n",
    "    # print(\"data is --->\",data)\n",
    "    conversation, spk0_time, spk1_time, sentence_wpm = conversation_data(data)\n",
    "   \n",
    "    spk0_content = {}\n",
    "    spk1_content = {}\n",
    "\n",
    "    \n",
    "    for key,val in conversation.items():\n",
    "        if val['speaker'] == 'spk_0':\n",
    "            spk0_content[key] = val['transcript']\n",
    "        else:\n",
    "            spk1_content[key] = val['transcript']\n",
    "            \n",
    "    # print(\"conversation -->\",conversation)  \n",
    "    print(spk0_content)\n",
    "    print(spk1_content)\n",
    "    \n",
    "    return conversation, spk0_content, spk1_content, spk0_time, spk1_time,sentence_wpm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_filler_words(text):\n",
    "    speech_text = ''\n",
    "    for key,value in text.items():\n",
    "        speech_text += value\n",
    "    speech_text = speech_text.split()\n",
    "\n",
    "    filler_words = [\"like\", \"um\", \"Um\", \"Um...\", \"Um..\", \"might\", \"aa\", \"aaa\", \"mean\", \"know\", \"well\", \"Well\", \"Oh\", \"oh\",\n",
    "                \"really\", \"basically\", \"oh\", \"maybe\", \"somehow\", \"Hi..\", \"Hi...\", \"Well\", \"like\", \"know\", \"mean\"]\n",
    "\n",
    "    total_filler_words = 0\n",
    "    for word in filler_words:\n",
    "        total_filler_words += speech_text.count(word)\n",
    "    percent = (total_filler_words / max(len(speech_text), 1)) * 100\n",
    "    return total_filler_words, percent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_speed(spk1_content, duration_secs):\n",
    "    speech_text = ''\n",
    "    for key,value in spk1_content.items():\n",
    "        speech_text += value\n",
    "    word_count = len(speech_text.split())\n",
    "    duration_min = duration_secs / 60\n",
    "    wpm = word_count / duration_min\n",
    "    speed = 'Slow'\n",
    "    if wpm > 70 and wpm <= 150:\n",
    "        speed = 'Medium'\n",
    "    elif wpm > 150 and wpm <= 200:\n",
    "        speed = 'Medium to Fast'\n",
    "    elif (wpm > 200):\n",
    "        speed = 'Fast'\n",
    "\n",
    "    return wpm, speed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_check(speaker_text):\n",
    "    client = Groq(api_key='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')\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"llama3-70b-8192\",\n",
    "      messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an English Grammar checker AI bot.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Your task is to check for significant grammatical errors, excluding punctuation errors, and identify any repetitions in the following sentences. \n",
    "                    Each sentence is indexed by a unique number and consider on unique indexed value at a time for {speaker_text}.\n",
    "\n",
    "                    transcript:\n",
    "                    {speaker_text}\n",
    "\n",
    "                    For each sentence, return the grammatical errors that a good speaker would typically avoid. Highlight the sentence with the error, specify the type of grammatical error, and provide the count of such errors. Also, identify any repetitive parts in the sentence, both in meaning and sentence structure.\n",
    "\n",
    "                    Based on the identified errors and repetitions, assign an English grammar quality score out of 5 using the following criteria:\n",
    "\n",
    "                    - 5/5: Ideal English speaker with minimal to no errors.\n",
    "                    - 4/5: Fluent English speaker with minor errors that do not hinder comprehension.\n",
    "                    - 3/5: Understandable but with noticeable errors and repetition that may affect clarity.\n",
    "                    - Less than 2/5: Significant grammatical errors that make comprehension difficult\n",
    "\n",
    "                    Note:\n",
    "                    - Please provide the final response in JSON format only. No additional text or explanations are needed.\n",
    "                    - Provide the results for each sentence separately.\n",
    "                    - Include a count of the significant grammatical errors and a count of repetitive words that a good speaker should avoid.\n",
    "                    - Include an overall English grammar quality score.\n",
    "\n",
    "                    Output Response:\n",
    "                    {{\n",
    "                        \"grammar_quality_score\": English grammar quality score out of 10,\n",
    "                        \"total_significant_errors\": Total number of significant grammatical errors,\n",
    "                        \"total_repetitions\": Total number of repetitive words,\n",
    "                        \n",
    "                        \"sentences\": {{\n",
    "                            \"number\": {{\n",
    "                                \"sentence\": \"The full sentence with the error.\",\n",
    "                                \"errors\": [\n",
    "                                    {{\n",
    "                                        \"error_type\": \"Type of grammatical error\",\n",
    "                                        \"count\": Number of occurrences\n",
    "                                    }}\n",
    "                                ],\n",
    "                                \"repetitive_parts\": [\n",
    "                                    \"List of repeated words or phrases\"\n",
    "                                ]\n",
    "                            }}\n",
    "                        }},\n",
    "                        \n",
    "                    }}\n",
    "                    \"\"\"\n",
    "                }\n",
    "            ]\n",
    "            ,\n",
    "            temperature=0.0,\n",
    "            max_tokens=2000,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "    text= completion.choices[0].message.content\n",
    "    data = json.loads(text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_similiarity_score(context, text1, text2):\n",
    "#     # goutam\n",
    "#     return \n",
    "def content_similarity(ideal_answer, spk0_content, spk1_content):\n",
    "    client = Groq(api_key='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')\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"llama3-70b-8192\",\n",
    "      messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a content similarity checker AI bot of an interview.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Evaluate the content similarity between the ideal answer and the candidate's answer.\n",
    "                    Question: {spk0_content}\n",
    "                    Ideal answer: {ideal_answer}\n",
    "                    Candidate's answer: {spk1_content}\n",
    "\n",
    "                    The task is to compare the relevance of the candidate's response to the ideal answer, focusing on the alignment of key concepts, completeness, clarity, coherence and accuracy.\n",
    "                    Provide a content similarity score out of ten, where ten indicates a perfect match and one indicates no relevance. \n",
    "                    Return the response only in JSON format as follows:\n",
    "                    Please provide the final response in JSON format only as follows. No additional text or explanations are needed.\n",
    "                        {{\n",
    "                            \"similarity_score\": \"<score>\",\n",
    "                            \"clarity_score\":\"<score>\",\n",
    "                            \"consistency_score\":\"<score>\"\n",
    "                            \"Coherence_score\":\"<score>\"\n",
    "                        }}\n",
    "                        \n",
    "                    where:\n",
    "                    \"\"clarity_score\": Ensure the text is clear and easy to understand. This involves checking for ambiguous phrases and ensuring the message is straightforward.\"\n",
    "                    \"consistency_score\": Check for consistent use of tense, point of view, and terminology throughout the text.\n",
    "                    \"coherence_score\": Look at how well the ideas flow together. Each sentence and paragraph should logically connect to the next.\n",
    "                    \"\"\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=2000,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "    text = completion.choices[0].message.content\n",
    "    data = json.loads(text)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideal_answer():\n",
    "    # text = \"JavaScript handles asynchronous operations, such as promises and timers, using the event loop. When a promise or timer is encountered, the synchronous code continues to execute, while the asynchronous operation is deferred. Once the operation is completed, the event loop pushes the callback to the call stack, ensuring that it runs after the current execution context is cleared. This allows JavaScript to manage non-blocking, asynchronous behavior efficiently.\"\n",
    "    # text = \"An interpreter is a program that directly executes instructions written in a programming language without requiring them to be compiled into machine code. It translates high-level code into an intermediate form, executing it line by line. This allows for easier debugging and immediate execution of code.\"\n",
    "    text = 'The CALCULATE function in Power BI changes the data context to apply specific filters to an expression. It helps you get results based on certain conditions. Think of it as a way to ask Power BI to “recalculate” something with new rules.'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(spk1_content, grammar_error):\n",
    "\n",
    "    text = ''\n",
    "    for key,val in spk1_content.items():\n",
    "        text += val\n",
    "    total_words = len(text.split())\n",
    "\n",
    "    grammatical_errors = grammar_error['total_significant_errors']\n",
    "    repetitions = grammar_error['total_repetitions']\n",
    "   \n",
    "    weight_grammar = 4\n",
    "    weight_repetition = 2\n",
    "    \n",
    "   \n",
    "    weighted_errors = (grammatical_errors * weight_grammar) + (repetitions * weight_repetition)\n",
    "    \n",
    "    max_possible_errors = total_words * (weight_grammar + weight_repetition)\n",
    "    score = 10 - (weighted_errors / max_possible_errors * 10)\n",
    "    \n",
    "    score = max(0, min(10, score))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition_score(repetitions, spk1_content):\n",
    "    text = ''\n",
    "    for key,val in spk1_content.items():\n",
    "        text += val\n",
    "    total_words = len(text.split())\n",
    "\n",
    "    score =  float(repetitions/total_words)*100\n",
    "\n",
    "    final_score = 10\n",
    "    if score <= 3:\n",
    "        final_score = 10\n",
    "    elif score > 3 and score <=5:\n",
    "        final_score = 9\n",
    "    elif score > 5 and score <=8:\n",
    "        final_score = 7.5\n",
    "    elif score >8 and score <=10:\n",
    "        final_score = 6\n",
    "    elif score >10 and score <=12:\n",
    "        final_score = 4.5\n",
    "    else:\n",
    "        final_score = 3\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # video_file_name = 'js_interview_clip1.mp4'\n",
    "    # transcription(video_file_name)\n",
    "    conversation, spk0_content, spk1_content, spk0_time, spk1_time,sentence_wpm  =  video_data()\n",
    "    # total_questions = len(spk0_content)\n",
    "    pprint(conversation)\n",
    "    \n",
    "\n",
    "    # total_filler_words, percent = find_filler_words(spk1_content)\n",
    "    # speech_rate, speed =  speech_speed(spk1_content, spk1_time)\n",
    "    \n",
    "    # idl_ans = ideal_answer()\n",
    "    # content_similarity_result = content_similarity(idl_ans, spk0_content, spk1_content)\n",
    "    # # print(content_similarity_result)\n",
    "    # ans_validation_result = content_similarity_result['similarity_score']\n",
    "    # clarity = content_similarity_result['clarity_score']\n",
    "    # coherence = content_similarity_result['coherence_score']\n",
    "    # grammar_error = grammar_check(spk1_content)\n",
    "    # grammar_score = calculate_score(spk1_content, grammar_error)\n",
    "    # optimal_repetetion = repetition_score(grammar_error['total_repetitions'],spk1_content)\n",
    "    # pprint(grammar_error)\n",
    "    # print(grammar_score)\n",
    "\n",
    "    # all_scores = {\n",
    "    #     'content': {\n",
    "    #         'total_questions': total_questions,\n",
    "    #         'ans_validation_score': ans_validation_result,\n",
    "    #         'quality':{  \n",
    "    #             'coherence': coherence,\n",
    "    #             'grammar' : grammar_score,\n",
    "    #             'optimal_repetetion' : optimal_repetetion ,\n",
    "    #             'clarity' : clarity,\n",
    "    #         },\n",
    "    #     'audio': {\n",
    "    #         'fluency' : 0,\n",
    "    #         'tone_and_clarity' : 0,\n",
    "    #     },\n",
    "    #     'video' : {\n",
    "    #         'eye_contact' : 0,\n",
    "    #         'expressions' : 0,\n",
    "    #     }\n",
    "    #     }\n",
    "    # }   \n",
    "    # pprint(all_scores)\n",
    "    # print(find_filler_words(spk1_content))\n",
    "    # print(speech_rate, speed)\n",
    "    # print(total_filler_words, percent)\n",
    "    # pprint(spk0_content)\n",
    "    \n",
    "    # print(spk1_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: interview-performance-analysis, Key: transcribed_files/better2.txt\n",
      "{'ResponseMetadata': {'RequestId': '8738Z4QX1FMCCKM7', 'HostId': 'fzuPsSnPFp+hU7O29OzU43+YcQtOwT3sMdBGyy8azxBzLtC/97PwF6vdqbBSOdxj/p9HXgVR+hIsgIs+k60BhA==', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'fzuPsSnPFp+hU7O29OzU43+YcQtOwT3sMdBGyy8azxBzLtC/97PwF6vdqbBSOdxj/p9HXgVR+hIsgIs+k60BhA==', 'x-amz-request-id': '8738Z4QX1FMCCKM7', 'date': 'Thu, 29 Aug 2024 12:19:43 GMT', 'last-modified': 'Tue, 27 Aug 2024 11:52:11 GMT', 'etag': '\"b719d873e0872f3a809c107fed04d300\"', 'x-amz-server-side-encryption': 'AES256', 'accept-ranges': 'bytes', 'content-type': 'binary/octet-stream', 'server': 'AmazonS3', 'content-length': '35502'}, 'RetryAttempts': 0}, 'AcceptRanges': 'bytes', 'LastModified': datetime.datetime(2024, 8, 27, 11, 52, 11, tzinfo=tzutc()), 'ContentLength': 35502, 'ETag': '\"b719d873e0872f3a809c107fed04d300\"', 'ContentType': 'binary/octet-stream', 'ServerSideEncryption': 'AES256', 'Metadata': {}, 'Body': <botocore.response.StreamingBody object at 0x00000224F7D78640>}\n",
      "{1: 'Just uh uh tell about the calculate function how it do and whatit does. So', 3: 'OK.'}\n",
      "{2: \"uh calculate function, it's the only function that helps to change the filter context on a measure when creating a measure. So uh it's essentially let's say if we have a visual there on a particular page, so we have a filter context coming from a slicer filter that forwill help us change the filter context and just supply our own, create our own filter context on a particular measure. So as soon as the measure is brought into our visual, uh we can just supply in the calculus. So it has multiple parameters like all this that so we can just create a measure and then uh create our own filter context on top of what is already there in the page.\"}\n",
      "{1: {'speaker': 'spk_0',\n",
      "     'transcript': 'Just uh uh tell about the calculate function how it do and '\n",
      "                   'whatit does. So'},\n",
      " 2: {'speaker': 'spk_1',\n",
      "     'transcript': \"uh calculate function, it's the only function that helps \"\n",
      "                   'to change the filter context on a measure when creating a '\n",
      "                   \"measure. So uh it's essentially let's say if we have a \"\n",
      "                   'visual there on a particular page, so we have a filter '\n",
      "                   'context coming from a slicer filter that forwill help us '\n",
      "                   'change the filter context and just supply our own, create '\n",
      "                   'our own filter context on a particular measure. So as soon '\n",
      "                   'as the measure is brought into our visual, uh we can just '\n",
      "                   'supply in the calculus. So it has multiple parameters like '\n",
      "                   'all this that so we can just create a measure and then uh '\n",
      "                   'create our own filter context on top of what is already '\n",
      "                   'there in the page.'},\n",
      " 3: {'speaker': 'spk_0', 'transcript': 'OK.'}}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
